{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here compute the TF-IDF on a corpus of newspaper headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data into the file *headlines.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset\n",
    "df = pd.read_csv(\"headlines.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, check the dataset basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170721</td>\n",
       "      <td>algorithms can make decisions on behalf of fed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170721</td>\n",
       "      <td>andrew forrests fmg to appeal pilbara native t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170721</td>\n",
       "      <td>a rural mural in thallan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australia church risks becoming haven for abusers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australian company usgfx embroiled in shanghai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20170721  algorithms can make decisions on behalf of fed...\n",
       "1      20170721  andrew forrests fmg to appeal pilbara native t...\n",
       "2      20170721                           a rural mural in thallan\n",
       "3      20170721  australia church risks becoming haven for abusers\n",
       "4      20170721  australian company usgfx embroiled in shanghai..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1999 entries, 0 to 1998\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   publish_date   1999 non-null   int64 \n",
      " 1   headline_text  1999 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# TODO: Have a look at the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publish_date      int64\n",
       "headline_text    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform preprocessing on this text data: tokenization, punctuation and stop words removal and stemming.\n",
    "\n",
    "Hint: to do so, use NLTK, *pandas*'s method *apply*, lambda functions and list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\61406\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\61406\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\61406\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\61406\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\61406\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         [algorithm, make, decis, behalf, feder, minist]\n",
       "1       [andrew, forrest, fmg, appeal, pilbara, nativ,...\n",
       "2                                 [rural, mural, thallan]\n",
       "3                  [australia, church, risk, becom, abus]\n",
       "4       [australian, compani, usgfx, embroil, shanghai...\n",
       "                              ...                        \n",
       "1994    [constitut, avenu, win, top, prize, act, archi...\n",
       "1995                         [dark, mofo, number, crunch]\n",
       "1996    [david, petraeu, say, australia, must, firm, s...\n",
       "1997    [driverless, car, australia, face, challeng, r...\n",
       "1998               [drug, compani, criticis, price, hike]\n",
       "Name: preprocessed_hline, Length: 1999, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Perform preprocessing\n",
    "#headline_df = df[\"headline_text\"]\n",
    "\n",
    "# import needed modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet') \n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Tokenize\n",
    "df['preprocessed_hline'] = df.apply(lambda df: nltk.word_tokenize(df[\"headline_text\"]), axis=1)\n",
    "\n",
    "# Remove punctuation\n",
    "df[\"preprocessed_hline\"] = df['preprocessed_hline'].apply(lambda tokens: [word for word in tokens if word.isalpha()])\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = stopwords.words('english')\n",
    "df['preprocessed_hline'] = df[\"preprocessed_hline\"].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "\n",
    "# Stem\n",
    "stemmer = PorterStemmer()\n",
    "df[\"preprocessed_hline\"] = df['preprocessed_hline'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n",
    "\n",
    "df[\"preprocessed_hline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>preprocessed_hline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170721</td>\n",
       "      <td>algorithms can make decisions on behalf of fed...</td>\n",
       "      <td>[algorithm, make, decis, behalf, feder, minist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170721</td>\n",
       "      <td>andrew forrests fmg to appeal pilbara native t...</td>\n",
       "      <td>[andrew, forrest, fmg, appeal, pilbara, nativ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170721</td>\n",
       "      <td>a rural mural in thallan</td>\n",
       "      <td>[rural, mural, thallan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australia church risks becoming haven for abusers</td>\n",
       "      <td>[australia, church, risk, becom, abus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australian company usgfx embroiled in shanghai...</td>\n",
       "      <td>[australian, compani, usgfx, embroil, shanghai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text  \\\n",
       "0      20170721  algorithms can make decisions on behalf of fed...   \n",
       "1      20170721  andrew forrests fmg to appeal pilbara native t...   \n",
       "2      20170721                           a rural mural in thallan   \n",
       "3      20170721  australia church risks becoming haven for abusers   \n",
       "4      20170721  australian company usgfx embroiled in shanghai...   \n",
       "\n",
       "                                  preprocessed_hline  \n",
       "0    [algorithm, make, decis, behalf, feder, minist]  \n",
       "1  [andrew, forrest, fmg, appeal, pilbara, nativ,...  \n",
       "2                            [rural, mural, thallan]  \n",
       "3             [australia, church, risk, becom, abus]  \n",
       "4  [australian, compani, usgfx, embroil, shanghai...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute now the Bag of Words for our data, using scikit-learn.\n",
    "\n",
    "Warning: since we used our own preprocessing, you have to bypass analyzer with identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the BOW of the preprocessed data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vec = CountVectorizer(stop_words = \"english\", lowercase=False, analyzer=lambda x: x)\n",
    "BOW = count_vec.fit_transform(df[\"preprocessed_hline\"]).toarray()\n",
    "\n",
    "BOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 4165)\n"
     ]
    }
   ],
   "source": [
    "print(BOW.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the shape of the BOW, the expected value is `(1999, 4165)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the Term Frequency and then the Inverse Document Frequency, and check the values are not only zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\61406\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.08333333, 0.09090909, 0.1       , 0.11111111,\n",
       "       0.125     , 0.14285714, 0.16666667, 0.18181818, 0.2       ,\n",
       "       0.22222222, 0.25      , 0.28571429, 0.33333333, 0.4       ,\n",
       "       0.5       , 1.        ])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the TF using the BOW\n",
    "BOW_df = pd.DataFrame(data = BOW, columns=count_vec.get_feature_names())\n",
    "term_freq = BOW_df\n",
    "term_freq = term_freq.divide(term_freq.sum(axis=1), axis=0)\n",
    "\n",
    "np.unique(term_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.28291422, 3.36629583, 3.44151925, 3.53995932, 3.57505064,\n",
       "       3.70858204, 3.79373984, 3.83920222, 3.91152288, 3.96281617,\n",
       "       4.04505427, 4.10389477, 4.13466643, 4.16641513, 4.19920495,\n",
       "       4.2331065 , 4.26819782, 4.30456547, 4.3423058 , 4.38152651,\n",
       "       4.4223485 , 4.46490812, 4.50935988, 4.5558799 , 4.60467006,\n",
       "       4.65596336, 4.71003058, 4.76718899, 4.82781361, 4.89235213,\n",
       "       4.961345  , 5.03545298, 5.11549568, 5.20250706, 5.29781724,\n",
       "       5.40317776, 5.52096079, 5.65449219, 5.80864287, 5.99096442,\n",
       "       6.21410797, 6.50179005, 6.90725515, 7.60040233])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the IDF\n",
    "IDF = BOW_df\n",
    "IDF[IDF > 1] = 1\n",
    "\n",
    "#method to avoid divide by zero\n",
    "#IDF = np.log((1 + len(IDF)) / (1 + IDF.sum(axis=0)))\n",
    "IDF = np.log((len(IDF)) / (IDF.sum(axis=0)))\n",
    "\n",
    "np.unique(IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute finally the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the TF-IDF\n",
    "TF_IDF = term_freq * IDF.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 10 words with the highest and lowest TF-IDF on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest TF-IDF on Average:\n",
      " gcfc    0.633367\n",
      "geel    0.633367\n",
      "gw      0.633367\n",
      "haw     0.633367\n",
      "melb    0.633367\n",
      "coll    0.633367\n",
      "adel    0.633367\n",
      "syd     0.633367\n",
      "nmfc    0.633367\n",
      "cold    0.690456\n",
      "dtype: float64\n",
      "\n",
      "Highest TF-IDF on Average:\n",
      " date         3.800201\n",
      "mongolian    3.800201\n",
      "puffbal      3.800201\n",
      "mous         3.800201\n",
      "rig          3.800201\n",
      "superannu    3.800201\n",
      "aquapon      3.800201\n",
      "loophol      3.800201\n",
      "pump         6.907255\n",
      "peacemak     7.600402\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
    "tf_idf_avg = TF_IDF.max(axis=0).sort_values()\n",
    "print(\"Lowest TF-IDF on Average:\\n\", tf_idf_avg[:10])\n",
    "print(\"\\nHighest TF-IDF on Average:\\n\", tf_idf_avg[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the TF-IDF using scikit-learn on our preprocessed data (the one you used to compute the BOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the TF-IDF using scikit learn\n",
    "# Import the module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate the TF-IDF vectorizer\n",
    "TFIDF_vec = TfidfVectorizer(lowercase = False, analyzer = lambda x: x)\n",
    "\n",
    "# Compute the TF-IDF\n",
    "TF_IDF_sk = TFIDF_vec.fit_transform(df[\"preprocessed_hline\"]).toarray()\n",
    "TF_IDF_sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the 10 highest and lowest TF-IDF words on average to the ones you had by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest TF-IDF (sk-learn method) on Average:\n",
      " coll     0.305258\n",
      "gw       0.305258\n",
      "nmfc     0.305258\n",
      "adel     0.305258\n",
      "melb     0.305258\n",
      "syd      0.305258\n",
      "haw      0.305258\n",
      "geel     0.305258\n",
      "gcfc     0.305258\n",
      "fabio    0.322574\n",
      "dtype: float64\n",
      "\n",
      "Highest TF-IDF (sk-learn method) on Average:\n",
      " mosul        0.779137\n",
      "rig          0.786813\n",
      "travel       0.788050\n",
      "aquapon      0.794899\n",
      "date         0.794899\n",
      "employ       0.795060\n",
      "financ       0.803629\n",
      "mongolian    0.831769\n",
      "pump         1.000000\n",
      "peacemak     1.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\61406\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "IT_IDF_skdf = pd.DataFrame(data = TF_IDF_sk, columns=TFIDF_vec.get_feature_names())\n",
    "it_idf_sk_avg = IT_IDF_skdf.max(axis=0).sort_values()\n",
    "\n",
    "print(\"Lowest TF-IDF (sk-learn method) on Average:\\n\", it_idf_sk_avg[:10])\n",
    "print(\"\\nHighest TF-IDF (sk-learn method) on Average:\\n\", it_idf_sk_avg[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have the same words? How do you explain it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the words are very different. This is due to the method differences in formula and normalisation techniques. The custom calculation method (method 1) used uses the general formula. This includes IDFusing binary values between 0 and 1, while replacing all values over 1 to just 1. This could shift values in the output. SK-Learn might also use a differrent normalisation technique of data. It often uses L2norm, which can effect the scale of TF IDF values. Scikit learn also usually uses a smoothing value of +1 to the term frequency as sublinear scaling, replacing it with 1 + log(TF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
